
## INTRODUCTION TO THE MODULE

A key ingredient for successful results in reinforcement learning is the right reward function. As we’ve learned, it’s hard to get this right for reasons including complexity and sparse feedback. Reinforcement learning with human feedback (RLHF) is a modern approach that has been developed to address this challenge. Humans are repeatedly provided two proposals and they select which is preferred. These preferences are used to construct a reward model. In this module, we will study RLHF, its strengths, and its limitations.

## LEARNING OUTCOMES

At the conclusion of this module, you should be able to:

- Explain RLHF, its strengths, and limitations
- Discuss a reward model and how it is used in RLHF
- Explain supervised fine tuning