{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (MDPs)\n",
    "\n",
    "Last updated: March 15, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 3\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand the properties of Markov Decision Processes\n",
    "- Describe the gains process and the agent's objective in RL\n",
    "- Explain optimal policy and value functions\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Markov Process\n",
    "- Markov Decision Process\n",
    "- Estimating the value of each action in each state\n",
    "- Estimating the value of each state given optimal action selection\n",
    "- optimal policy\n",
    "- optimal state value function, optimal action value function\n",
    "- discounting\n",
    "- gains process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the Markov Decision Process (MDP) Formalism\n",
    "\n",
    "In reinforcement learning, time-varying state spaces are often modeled with Markov Decision Processes (MDP).  \n",
    "\n",
    "For each time step *t*, an agent observes the current state of the environment $s_t$,  \n",
    "takes action $a_t$, receives reward $r_t$, and transitions to new state $s_{t+1}$.\n",
    "\n",
    "MDPs originate from field of *optimal control*\n",
    "\n",
    "Some important properties:\n",
    "\n",
    "- they can model sequential decision-making\n",
    "- the agent can be in a set of different states (finite or continuous)\n",
    "- actions can be discrete or continuous\n",
    "- the rewards are short-term, but collectively result in long-term value\n",
    "\n",
    "A word about the **Markov** property:\n",
    "\n",
    "This means that the future state will depend only on the current state. These sorts of processes are much easier to model.  \n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P,R)$\n",
    "\n",
    "$S$ is the state space  \n",
    "$A$ is the action space  \n",
    "$P$ is the probability transition matrix  \n",
    "$R$ is the reward\n",
    "\n",
    "Sometimes a discount factor $\\gamma$ is also included in the tuple.  \n",
    "Discounting future rewards is important for capturing time value of money, which is a finance concept.\n",
    "\n",
    "This diagram illustrates the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mdp](images/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals and Rewards\n",
    "\n",
    "Rewards are short-term in nature. **The agent will have this objective: maximize total expected discounted reward.**  \n",
    "It will attempt to learn the optimal policy (mapping states to actions) to reach this objective.  \n",
    "This is what it means to solve the reinforcement learning problem.\n",
    "\n",
    "To this end, we first define the *gains process* or *return* as a function of time.  \n",
    "This is the sum future rewards discounted to time $t$:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "Since reality is uncertain, we work with expected gains.  \n",
    "The agent attempts to maximize expected gains by discovering the best action from each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policies and Value Functions\n",
    "\n",
    "Denote policy as $\\pi$\n",
    "\n",
    "A policy is a mapping from each state to probabilities of selecting each action\n",
    "\n",
    "It might be that each state maps to a single action with probability 1 (no uncertainty)\n",
    "\n",
    "The optimal policy is denoted $\\pi_*$\n",
    "\n",
    "**state-value functions**\n",
    "\n",
    "It is important to understand the value of starting from a state and following a given policy.\n",
    "\n",
    "This is denoted $v_{\\pi}(s)$\n",
    "\n",
    "Definition using the gains process:\n",
    "\n",
    "$v_\\pi(s) = E_{\\pi}[G_t | S_t=s] = E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} | S_t=s]$ for all $s \\in S$\n",
    "\n",
    "**action-value functions**\n",
    "\n",
    "The agent will want to improve upon a policy. To do this, it must measure the value of  \n",
    "\n",
    "1) starting in state $s$  \n",
    "2) trying action $a$  \n",
    "3) following policy $\\pi$ for the remainder of the actions\n",
    "\n",
    "This quantity, which is fundamental in Q-Learning, is denoted $q_{\\pi}(s, a)$\n",
    "\n",
    "Definition using the gains process:\n",
    "\n",
    "$q_\\pi(s,a) =  E_{\\pi}[G_t | S_t=s, A_t=a] = E_{\\pi}[\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1} | S_t=s, A_t=a]$ for all $s \\in S$\n",
    "\n",
    "Later, we will see these value functions in a recursive form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent solves the reinforcement learning problem by finding the best policy $\\pi_*$\n",
    "\n",
    "This policy, which uses the best action from each state, will maximize reward in each state.\n",
    "\n",
    "The optimal state value function is defined as: \n",
    "\n",
    "$v_*(s) := \\underset{\\pi}{\\operatorname{\\max}} v_{\\pi}(s)$ for all states\n",
    "\n",
    "The optimal action value function is defined as: \n",
    "\n",
    "$q_*(s,a) := \\underset{\\pi}{\\operatorname{\\max}} q_{\\pi}(s,a)$\n",
    "\n",
    "Our efforts will be to learn these quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
