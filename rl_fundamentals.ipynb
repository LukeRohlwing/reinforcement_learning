{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental concepts of Reinforcement Learning\n",
    "\n",
    "Last updated: March 15, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 1\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand how reinforcement learning is different from other machine learning problems\n",
    "- Understand the key elements of RL\n",
    "- Distinguish between reward and value\n",
    "- Distinguish between exploration and exploitation, explaining the tradeoff in value discovery\n",
    "- Understand the difference between behavior policy and target policy\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Reinforcement learning\n",
    "- agent\n",
    "- environment\n",
    "- state\n",
    "- action\n",
    "- reward\n",
    "- exploration vs exploitation\n",
    "- behavior policy and target policy\n",
    "- $\\epsilon$-greedy strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential idea of Reinforcement Learning\n",
    "\n",
    "In this branch of machine learning, an *agent* learns by interacting with its *environment.*  \n",
    "The agent has a goal: to maximize a quantitative measure of *reward*.\n",
    "\n",
    "In many situations, the ground truth is not known:\n",
    "\n",
    "- a patient is treated for sepsis, a life-threatening emergency. what is the best treatment? \n",
    "- an investor must select from a universe of investments, managing portfolio risk and return over time. what is the best policy for dynamic allocation?\n",
    "\n",
    "In other situations, we may have access to stale ground truths. This is common in dynamic environments:\n",
    "\n",
    "- trends and levels before versus during a pandemic\n",
    "- adversarial environments like finance and fraud where agents adapt their behavior\n",
    "- patient health following injury or illness\n",
    "\n",
    "\n",
    "Contrast with Supervised Learning (SL) where ground truth is provided up front.\n",
    "\n",
    "Contrast with Unsupervised Learning (UL) where structure is uncovered from unlabeled data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions Play an Essential Role\n",
    "\n",
    "Modeling generally involves the state:\n",
    "\n",
    "- predict if a check is fraudulent\n",
    "- predict if a stock price will rise\n",
    "- predict if a patient will be readmitted\n",
    "\n",
    "An exciting addition in reinforcement learning:\n",
    "\n",
    "We want to make predictions and control systems **given a state and an action taken**:\n",
    "\n",
    "- what will happen if an agent takes action $a$ in state $s$?\n",
    "\n",
    "This is fundamentally different and very exciting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rl_graphic.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent and Environment**  \n",
    "\n",
    "The agent might be a person, a robot or other being that registers sensations from the \"external surroundings.\"\n",
    "\n",
    "Consider tracking a person's pulse. While pulse is generated in the body, it is measured by an external device. Pulse is considered part of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *state* reflects all relevant information sensed by the agent about its environment.  \n",
    "In patient health, this might be vital signs and medications.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *actions* represent the controls available to the agent from each state.  \n",
    "A robot might move one step left, right, forward or backward at each point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *reward* is essential in defining the problem goal.  \n",
    "After the agent executes a step, the environment sends the agent a reward, which is a number.  \n",
    "\n",
    "The reward is short term in nature.  \n",
    "Selecting actions based on a single reward (greedy actions) may be suboptimal is it may limit future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *policy* is a function that maps each state to an action. It represents how the agent will behave in each state.  \n",
    "The policy might be \n",
    "- deterministic (each action produces a single state)\n",
    "- stochastic (actions produce a distribution of state values). this case is more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *value function* represents the long-term value of a state.  \n",
    "This is quantified as the sum of all expected discounted rewards.\n",
    "\n",
    "Action choices are made based on value judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discounting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflecting time value of money, future rewards are discounted at rate $\\gamma$ per time step "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model of environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a state *s* and action *a*, the next state and reward need to be determined.  \n",
    "In some cases, a model is used to inform the agent of the result from taking action a in state s.  \n",
    "The model needs to accurately reflect the environment.\n",
    "\n",
    "Model-free approaches use trial and error. This works for video games but not healthcare, for example, where caregivers generally can't liberally experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploration vs Exploitation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true value of a state needs to be estimated and it follows a distribution. There is generally uncertainty.  \n",
    "As we take action to visit eacch state, we approximate the true values.  \n",
    "\n",
    "We might choose to stick with a state with estimated maximal value, not realizing it to be suboptimal.  \n",
    "Staying with the state is called *exploitation*. This is the greedy action, as the agent selects the highest short-term reward.  \n",
    "Acting greedily might limit the possibility for greater long-term gain, so it is important to explore early on.\n",
    "\n",
    "\n",
    "Trying the other states is called *exploration*.  \n",
    "\n",
    "There is a **tradeoff between exploitation and exploration:**  \n",
    "in the short run, exploration might produce lower rewards, but the hope is to learn the true maximal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavioral Policy vs Target Policy** \n",
    "\n",
    "Related to the concept of exploration vs exploitation are two important policies:\n",
    "\n",
    "- the *target policy* is the policy that an agent is trying to learn\n",
    "- the *behavior policy* is the policy being used by an agent \n",
    "\n",
    "To more efficiently learn, it can help the agent to follow the behavior policy (this amounts to exploring).\n",
    "\n",
    "Later we will talk about off-policy methods for learning, which uses the target and behavior policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\epsilon$-greedy Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable method for finding more valuable states is exploit for the majority of the time while exploring a small fraction of the time.\n",
    "\n",
    "In this strategy, the agent selects the action with maximal value (it exploits) with fraction 1-$\\epsilon$  \n",
    "and selects a random action with fraction $\\epsilon$ (it explores).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_values = [1, -1, 0, 2]\n",
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(epsilon, action_values):\n",
    "    action_size = len(action_values)\n",
    "    if np.random.rand() <= epsilon: # random draw with prob epsilon\n",
    "        return random.randrange(action_size)\n",
    "    return np.argmax(action_values)  # returns action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take action following $\\epsilon$-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act(epsilon, action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat 1000x and produce histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "plt.hist([act(epsilon, action_values) for i in range(1000)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**  \n",
    "Given $\\epsilon$=0.5, what fraction of the time do we expect to select the highest-valued action?  \n",
    "This would include selection by both chance and intention.  \n",
    "Try simulating this and checking if the result matches your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 0.5\n",
    "# simulate the values\n",
    "vals = [act(epsilon, action_values) for i in range(1000)]\n",
    "# compute fraction of actions = 3\n",
    "len([v for v in vals if v == 3]) / len(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 Soln:\n",
    "\n",
    "by intention: 50%  \n",
    "by chance:    (1/4) * 50%  \n",
    "total = 62.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
