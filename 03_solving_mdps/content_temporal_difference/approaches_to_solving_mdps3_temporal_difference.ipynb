{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Solving MDPs III: Temporal Difference\n",
    "\n",
    "Last updated: March 15, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 6\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand how the temporal-difference method makes updates\n",
    "- Explain how TD updates are an improvement over MC updates\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Temporal-Difference method\n",
    "- Temporal-Difference error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-Difference (TD) Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "![mc_vs_td](images/mc_vs_td.png)  \n",
    "**Illustration Comparing MC Updates to TD Updates**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of MC is that we must simulate a full trajectory before updating a policy\n",
    "\n",
    "TD methods learn from experience and can provide updates after each time step using *bootstrapping*  \n",
    "\n",
    "**Revisiting the commute time example**  \n",
    "if we realize mid-trip that we're running 30 minutes late, we can use this information  \n",
    "mid-trip to update the total commute time estimate.\n",
    "\n",
    "\n",
    "**What do we mean by bootstrapping?**  \n",
    "Basing an update on an existing estimate. The initial estimates are incorrect but useful as we can refine them.\n",
    "\n",
    "\n",
    "**TD(0)** is a method that learns after one time step  \n",
    "\n",
    "The method is mathematically sound and for any policy $\\pi$, the estimate of value function $v_\\pi$ converges\n",
    "\n",
    "Modern RL algorithms implement TD methods with function approximation - specifically neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD Prediction\n",
    "\n",
    "Let's start with state-value function of policy $\\pi$ starting from state *s*:\n",
    "\n",
    "$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a step from *s* under $\\pi$ produces action *a*, reward *r* and next state *s'*\n",
    "\n",
    "observed quantity $r + \\gamma v_\\pi(s')$ gives new estimate of $v_\\pi(s)$ based on one sample\n",
    "\n",
    "**Important idea: use this observation to update the existing value estimate by moving it closer to new observation** \n",
    "\n",
    "Don't want to discard old estimate, as new one may be noisy and it's sample size of one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Rule\n",
    "\n",
    "We update $v_\\pi(s)$ after a single step by computing the convex combination of its old estimate and the observed quantity.  \n",
    "A weight $\\alpha\\in (0,1]$ is applied for mixing the two, where larger value weights the recent observation more heavily.\n",
    "\n",
    "We will use $V$ to denote values which are based on sample data under policy $\\pi$.\n",
    "\n",
    "$V(s) := (1-\\alpha) V(s) + \\alpha (r + \\gamma V(s'))$ \n",
    "\n",
    "Notice: we now have a way to update the state value after a single transition based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD Error\n",
    "\n",
    "Let's rearrange the update equation:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{aligned}  \n",
    "V(s) &:= (1-\\alpha) V(s) + \\alpha (r + \\gamma V(s'))  \\\\\n",
    "&= V(s) + \\alpha [r + \\gamma V(s') -  V(s)]\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "This form make the update clear, and it presents a term $[r + \\gamma V(s') -  V(s)]$ which we call *TD error*.  \n",
    "\n",
    "We saw an update form like this earlier. A similar form appears in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**  \n",
    "Think about the TD error and how it works. Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**  \n",
    "What happens when $\\alpha=0$? What happens when $\\alpha=1$?  \n",
    "Is it a good idea to use these values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have the environment dynamics. To improve the policy, we need estimate of the action values $q(s,a)$.\n",
    "\n",
    "As a reminder, $q_\\pi(s,a)$ represents starting in state $s$, taking action $a$ and subsequently following policy $\\pi$.\n",
    "\n",
    "Why do we take action $a$? This helps us try to improve the policy.\n",
    "\n",
    "\n",
    "\n",
    "A popular on-policy method is *Sarsa*.\n",
    "\n",
    "A popular off-policy method is *Q-learning*, and this method (and related methods) are most popular.\n",
    "\n",
    "We will discuss *Q-learning* going forward. Taking TD(0) update steps will be essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
