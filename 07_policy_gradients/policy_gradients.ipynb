{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "### Last updated: October 9, 2023\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 13\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand policy gradient methods\n",
    "- Understand a policy gradient algorithm: REINFORCE\n",
    "- Understand extensions of REINFORCE: why is a baseline useful\n",
    "- Understand why a critic is useful\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- policy gradient methods\n",
    "- parametrizing the policy\n",
    "- baseline\n",
    "- actor-critic methods\n",
    "\n",
    "### ORDERING\n",
    "\n",
    "To follow Deep Q-Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Policy-Based Methods\n",
    "\n",
    "Most of our work has been with value functions (e.g., action-value estimates). \n",
    "\n",
    "This is an indirect yet powerful way to tackle sequential prediction and control problems\n",
    "\n",
    "Now we do something different: **directly model the policy and work to improve it**\n",
    "\n",
    "What does this buy us?\n",
    "\n",
    "- arguably more principled since policy methods directly optimize the policy parameters  \n",
    "  this is more efficient in some cases\n",
    "\n",
    "- allows us to model continuous action spaces\n",
    "\n",
    "- can learn truly random stochastic policies  \n",
    "  in some cases the optimal policy is a mix of two actions with fixed probabilities (bluff or not in Poker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy is parametrized**\n",
    "\n",
    "Denote the *policy parameter* $\\boldsymbol{\\theta}$ which is a vector\n",
    "\n",
    "The policy is parametrized as: \n",
    "\n",
    "$\\pi(a|s,\\boldsymbol{\\theta}) = P(A_t=a | S_t=s, \\boldsymbol{\\theta_t}=\\boldsymbol{\\theta})$\n",
    "\n",
    "The parametrization is flexible, but $\\pi(a|s,\\boldsymbol{\\theta})$ must be differentiable wrt $\\boldsymbol{\\theta}$\n",
    "\n",
    "The goal is to learn how to update the parameter components to improve performance measure $J$\n",
    "\n",
    "**Gradient Ascent to make improvements**\n",
    "\n",
    "We will follow the direction of the gradient to improve $\\boldsymbol{\\theta}$\n",
    "\n",
    "The update equation has form:\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha \\hat{\\nabla  J(\\boldsymbol{\\theta_t})} $\n",
    "\n",
    "where \n",
    "\n",
    "- $\\alpha$ is the step size (learning rate)\n",
    "\n",
    "- $\\hat{\\nabla  J(\\boldsymbol{\\theta_t})}$ is a stochastic estimate of the gradient of the performance measure with respect to the parameters. \n",
    "\n",
    "On average, $\\hat{\\nabla  J(\\boldsymbol{\\theta_t})}$ should approach the true gradient.\n",
    "\n",
    "It is common to use a softmax distribution for the policy:\n",
    "\n",
    "$\\pi(a|s,\\boldsymbol{\\theta}) = \\frac{\\exp{h(s,a,\\boldsymbol{\\theta})}}{\\sum_b \\exp{h(s,a,\\boldsymbol{\\theta})}}$\n",
    "\n",
    "where $h(s,a,\\boldsymbol{\\theta})$ is a parametrized numerical preference given state $s$ and action $a$.\n",
    "\n",
    "higher preference leads to higher probability\n",
    "\n",
    "This is where modeling comes in, as $h(s,a,\\boldsymbol{\\theta})$ could be any sort of model (linear, deep neural network, ...).\n",
    "\n",
    "Methods following this approach are called are policy gradient (PG) methods.\n",
    "\n",
    "Methods that learn approximations to policy and value functions are called *actor-critic methods*.  \n",
    "*Actor*: the learned policy  \n",
    "*Critic*: the learned value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Our First Policy-Gradient Algorithm: REINFORCE\n",
    "\n",
    "REINFORCE is a Monte Carlo Policy Gradient algorithm\n",
    "\n",
    "Need to sample such that the expectation of the sample gradient is proportional to population gradient. \n",
    "\n",
    "The policy gradient theorem (see Sutton & Barto Ch 13 for details) gives an expression to obtain unbiased samples.\n",
    "\n",
    "The update equation (excludes discounting), which uses stochastic gradient ascent, has form:\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha G_t \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta_t})}{\\pi(A_t|S_t, \\boldsymbol{\\theta_t})} $\n",
    "\n",
    "where $G_t$ is the return of the trajectory\n",
    "\n",
    "Since each trajectory must be fully simulated to compute $G_t$, this is a Monte Carlo method.\n",
    "\n",
    "**The intuition behind the equation:**\n",
    "\n",
    "- the parameter component update is proportional to two factors:  \n",
    "  1) the product of the return   \n",
    "  2) the gradient of the probability of taking the action divided by the probability of taking the action\n",
    "\n",
    "- $G_t$ in the numerator will act to directly increase the update in that direction\n",
    "- the probability of action $A_t$ in the denominator will act to decrease the update in that direction\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: \n",
    "- differentiable policy $\\pi(a|s,\\boldsymbol{\\theta})$\n",
    "- step size $\\alpha$\n",
    "- policy parameter $\\boldsymbol{\\theta_t}$\n",
    "\n",
    "Initialize $\\boldsymbol{\\theta_t}$\n",
    "\n",
    "Loop for each episode:  \n",
    "- Generate an episode $S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_T$  \n",
    "- Loop for each time step $t=0,1,...,T-1$:\n",
    "\n",
    "    - $ G = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$  (compute the discounted return of the trajectory)\n",
    "    \n",
    "    - $ \\boldsymbol{\\theta} = \\boldsymbol{\\theta} + \\alpha \\gamma^t G \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta})}{\\pi(A_t|S_t, \\boldsymbol{\\theta})} $ (update the parameter vector using stochastic gradient ascent)\n",
    "\n",
    "where the parameter update equation uses $\\gamma^t$ for discounting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE can have high variance and converge slowly.\n",
    "\n",
    "Can be generalized to include a comparison of the action value to a baseline $b(s)$\n",
    "\n",
    "The baseline can reduce variance and speed up learning, without introducing bias\n",
    "\n",
    "Here is the update rule (excludes discounting):\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha (G_t - b(s)) \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta_t})}{\\pi(A_t|S_t, \\boldsymbol{\\theta_t})} $\n",
    "\n",
    "A useful baseline is an estimate of the state value $\\hat{v}(S_t, \\textbf{w})$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is very similar to REINFORCE but with these differences:\n",
    "- introduce baseline function as state-value function\n",
    "- step size for policy update and value function update\n",
    "- compute baseline\n",
    "- update equation for weights in state-vaue function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reinforce](./reinforce_w_baseline0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Step Actor-Critic\n",
    "\n",
    "REINFORCE with baseline uses state-value function estimates based only on first state of each transition\n",
    "\n",
    "In actor-critic methods, the state-value function is also applied to the second state transition.  \n",
    "This allows for computing a one-step return, which can assess the action (acts as *critic*).\n",
    "\n",
    "Overall policy-gradient method called *actor-critic* method.\n",
    "\n",
    "This one-step method replaces full return of REINFORCE with one-step return. It is fully online and incremental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ac_one_step](./actor_critic_one_step0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going Deeper**\n",
    "\n",
    "We will dive further into Actor-Critic and other extensions next.  \n",
    "\n",
    "If you are interested in policy gradients applied to precision medicine:\n",
    "\n",
    "*Precision medicine as a control problem: Using simulation and deep reinforcement learning to discover adaptive, personalized multi-cytokine therapy for sepsis.* \n",
    "\n",
    "Brenden K. Petersen, Jiachen Yang, Will S. Grathwohl, Chase Cockrell, Claudio Santiago, Gary An, Daniel M. Faissol\n",
    "\n",
    "https://arxiv.org/abs/1802.10440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
